{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/milad/projects/medical-nlp-pipeline'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_prep(row):\n",
    "    label_dict = {'Medical Necessity': 0, 'Experimental/Investigational': 1, 'Urgent Care': 2}\n",
    "    label_type = label_dict[row['Type']]\n",
    "    return {'labels': label_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(path='csv',data_files='artifacts/data_ingestion/ca-independent-medical-review/Independent_Medical_Reviews_Custom.csv',split=Split.TRAIN)\n",
    "dataset = dataset.map(label_prep)\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "split_dataset = shuffled_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Findings', 'Type', 'labels'],\n",
       "        num_rows: 15380\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Findings', 'Type', 'labels'],\n",
       "        num_rows: 3845\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Findings': Value(dtype='string', id=None),\n",
       " 'Type': Value(dtype='string', id=None),\n",
       " 'labels': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping integer labels to string labels and vv\n",
    "id2label = {0: 'Medical Necessity', 1: 'Experimental/Investigational', 2: 'Urgent Care'}\n",
    "label2id = {'Medical Necessity': 0, 'Experimental/Investigational': 1, 'Urgent Care': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milad/miniconda3/envs/medical/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "bert_cls = DistilBertForSequenceClassification.from_pretrained(\n",
    "'distilbert-base-uncased', num_labels=len(id2label), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_tokenizer = AutoTokenizer.from_pretrained(\n",
    "'distilbert-base-uncased'\n",
    ")\n",
    "def tokenize(row):\n",
    "    return auto_tokenizer(row['Findings'],\n",
    "                        truncation=True,\n",
    "                        padding='max_length',\n",
    "                        max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/3845 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3845/3845 [00:00<00:00, 4111.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "tokenize, batched=True\n",
    ")\n",
    "tokenized_test_dataset = test_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Findings': 'Nature of Statutory Criteria/Case Summary: A male enrollee has requested Zepatier for treatment of his hepatitis C virus. Findings: The physician reviewer found that according to the recent joint guidelines issued by the American Association for the Study of Liver Diseases (AASLD) and the Infectious Diseases Society of America (IDSA), all patients with chronic hepatitis C should be treated except those with limited life expectancy due to non-liver-related conditions. After failure of NS5A inhibitors such as Harvoni, AASLD/IDSA guidelines state that patients who are high priority for treatment should undergo testing for resistance-associated variants before re-treatment. No NS5A or NS3 inhibitor resistance testing results have been provided in this case. If there is no NS5A resistance, Zepatier is not recommended.  When resistance to NS5A inhibitors is present, the AASLD/IDSA guidelines do not recommend use of Zepatier therapy unless there is also NS3 resistance.  If both types of resistance are present, Zepatier should not be used alone. Alternative therapies are recommended in most cases. Based on the information provided, the available medical literature does not support the use of Zepatier in this case.  Final Result: The reviewer determined that the requested medication is not medically necessary for treatment of the patient’s medical condition. Therefore, the Health Plan’s denial should be upheld.Credentials/Qualifications: The physician reviewer is board certified in internal medicine with sub-specialty certification in gastroenterology and is actively practicing. The reviewer is an expert in the treatment of the enrollee’s medical condition and knowledgeable about the proposed treatment through recent or current actual clinical experience treating those with the same or a similar medical condition.', 'Type': 'Medical Necessity', 'labels': 0, 'input_ids': [101, 3267, 1997, 15201, 9181, 1013, 2553, 12654, 1024, 1037, 3287, 25612, 4402, 2038, 7303, 27838, 24952, 2121, 2005, 3949, 1997, 2010, 28389, 1039, 7865, 1012, 9556, 1024, 1996, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset.set_format(\n",
    "type='torch',\n",
    "columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "tokenized_test_dataset.set_format(\n",
    "type='torch',\n",
    "columns=['input_ids', 'attention_mask', 'labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(model=bert_cls,\n",
    "train_dataset=tokenized_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=300,\n",
    "    logging_steps=300,\n",
    "    gradient_accumulation_steps=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=bert_cls,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_train_dataset,\n",
    "                eval_dataset=tokenized_test_dataset,\n",
    "                compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 300/960 [01:17<02:46,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3637, 'grad_norm': 1.3411959409713745, 'learning_rate': 3.4375e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 31%|███▏      | 301/960 [01:21<14:34,  1.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2644086480140686, 'eval_accuracy': 0.8925877763328999, 'eval_runtime': 3.623, 'eval_samples_per_second': 1061.282, 'eval_steps_per_second': 132.764, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 600/960 [02:40<01:10,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.234, 'grad_norm': 2.3393137454986572, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 63%|██████▎   | 601/960 [02:44<07:37,  1.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25135377049446106, 'eval_accuracy': 0.893368010403121, 'eval_runtime': 3.6055, 'eval_samples_per_second': 1066.436, 'eval_steps_per_second': 133.409, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 900/960 [03:59<00:14,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2076, 'grad_norm': 2.560081720352173, 'learning_rate': 3.125e-06, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 94%|█████████▍| 900/960 [04:02<00:14,  4.23it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23597051203250885, 'eval_accuracy': 0.9076723016905072, 'eval_runtime': 3.5624, 'eval_samples_per_second': 1079.322, 'eval_steps_per_second': 135.021, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 960/960 [04:18<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 258.3223, 'train_samples_per_second': 119.076, 'train_steps_per_second': 3.716, 'train_loss': 0.2630216340223948, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=960, training_loss=0.2630216340223948, metrics={'train_runtime': 258.3223, 'train_samples_per_second': 119.076, 'train_steps_per_second': 3.716, 'total_flos': 238445569843200.0, 'train_loss': 0.2630216340223948, 'epoch': 1.9973992197659298})"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/481 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 481/481 [00:03<00:00, 128.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.23864923417568207,\n",
       " 'eval_accuracy': 0.9063719115734721,\n",
       " 'eval_runtime': 3.752,\n",
       " 'eval_samples_per_second': 1024.775,\n",
       " 'eval_steps_per_second': 128.197,\n",
       " 'epoch': 1.9973992197659298}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('artifacts/training/distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "loaded_model = (AutoModelForSequenceClassification.from_pretrained('artifacts/training/bert-base-uncased'))\n",
    "loaded_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loaded_model.to(device)\n",
    "loaded_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '''Nature of Statutory Criteria/Case Summary:  An enrollee has requested a vascular surgery evaluation for treatment of her bilateral lower extremity varicose veins.  \n",
    "Findings:  The physician reviewer found that based on the available documentation, the requested vascular surgery evaluation is medically necessary to manage this patient’s compression hose, \n",
    "to rule out other causes for pain other than varicose veins, and to interpret diagnostic imaging in the presence of varicose veins. Further, the requested evaluation is consistent with the recommended management and treatment guidelines in the current medical literature. \n",
    "Thus, the requested evaluation with a vascular surgeon is supported as medically necessary for evaluation of this patient’s painful varicose veins.  \n",
    "Final Result: The reviewer determined that the requested services are medically necessary for evaluation of the patient’s medical condition. \n",
    "Therefore, the Health Plan’s denial should be overturned.  Credentials/Qualifications: The reviewer is board certified in surgery with sub-specialty certification in vascular surgery and is actively practicing. \n",
    "The reviewer is an expert in the treatment of the enrollee’s medical condition and knowledgeable about the proposed treatment through recent or current actual clinical experience treating those with the same or a similar medical condition.'''\n",
    "tokens = auto_tokenizer(sentence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  3267,  1997, 15201,  9181,  1013,  2553, 12654,  1024,  2019,\n",
       "         25612,  4402,  2038,  7303,  1037, 21449,  5970,  9312,  2005,  3949,\n",
       "          1997,  2014, 17758,  2896,  4654,  7913, 16383, 13075, 11261,  3366,\n",
       "          9607,  1012,  9556,  1024,  1996,  7522, 12027,  2179,  2008,  2241,\n",
       "          2006,  1996,  2800, 12653,  1010,  1996,  7303, 21449,  5970,  9312,\n",
       "          2003,  2966,  2135,  4072,  2000,  6133,  2023,  5776,  1521,  1055,\n",
       "         13379, 21290,  1010,  2000,  3627,  2041,  2060,  5320,  2005,  3255,\n",
       "          2060,  2084, 13075, 11261,  3366,  9607,  1010,  1998,  2000, 17841,\n",
       "         16474, 12126,  1999,  1996,  3739,  1997, 13075, 11261,  3366,  9607,\n",
       "          1012,  2582,  1010,  1996,  7303,  9312,  2003,  8335,  2007,  1996,\n",
       "          6749,  2968,  1998,  3949, 11594,  1999,  1996,  2783,  2966,  3906,\n",
       "          1012,  2947,  1010,  1996,  7303,  9312,  2007,  1037, 21449,  9431,\n",
       "          2003,  3569,  2004,  2966,  2135,  4072,  2005,  9312,  1997,  2023,\n",
       "          5776,  1521,  1055,  9145, 13075, 11261,  3366,  9607,  1012,  2345,\n",
       "          2765,  1024,  1996, 12027,  4340,  2008,  1996,  7303,  2578,  2024,\n",
       "          2966,  2135,  4072,  2005,  9312,  1997,  1996,  5776,  1521,  1055,\n",
       "          2966,  4650,  1012,  3568,  1010,  1996,  2740,  2933,  1521,  1055,\n",
       "         14920,  2323,  2022, 17068,  1012, 22496,  1013, 15644,  1024,  1996,\n",
       "         12027,  2003,  2604,  7378,  1999,  5970,  2007,  4942,  1011, 12233,\n",
       "         10618,  1999, 21449,  5970,  1998,  2003,  8851, 12560,  1012,  1996,\n",
       "         12027,  2003,  2019,  6739,  1999,  1996,  3949,  1997,  1996, 25612,\n",
       "          4402,  1521,  1055,  2966,  4650,  1998,  3716,  3085,  2055,  1996,\n",
       "          3818,  3949,  2083,  3522,  2030,  2783,  5025,  6612,  3325, 12318,\n",
       "          2216,  2007,  1996,  2168,  2030,  1037,  2714,  2966,  4650,  1012,\n",
       "           102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]])}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.to(loaded_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9107, -1.9434, -3.1365]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.eval()\n",
    "logits = loaded_model(input_ids=tokens['input_ids'],\n",
    "attention_mask=tokens['attention_mask'])\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.logits.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Medical Necessity\n"
     ]
    }
   ],
   "source": [
    "predicted_class_idx = logits.logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", loaded_model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    nlp_trained_model_path: Path\n",
    "    nlp_updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    # mlflow_uri: str\n",
    "    all_params: dict\n",
    "    params_batch_size: int\n",
    "    params_epochs: int\n",
    "    params_learning_rate: float\n",
    "    params_model_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from medical_nlp.constants import *\n",
    "from medical_nlp.utils.common import read_yaml, create_directories\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# MLFLOW_TRACKING_URI = os.environ[\"MLFLOW_TRACKING_URI\"]\n",
    "# MLFLOW_TRACKING_USERNAME = os.environ[\"MLFLOW_TRACKING_USERNAME\"]\n",
    "# MLFLOW_TRACKING_PASSWORD = os.environ[\"MLFLOW_TRACKING_PASSWORD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class configurationManager:\n",
    "    def __init__(self,\n",
    "                 config_file_path=CONFIG_FILE_PATH,\n",
    "                 params_file_path=PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        model_training = self.config.model_training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        training_data = os.path.join(self.config.data_ingestion.root_dir, 'ca-independent-medical-review/')\n",
    "        \n",
    "        create_directories([model_training.root_dir])\n",
    "        \n",
    "        training_config = TrainingConfig(\n",
    "            root_dir= model_training.root_dir,\n",
    "            nlp_trained_model_path= model_training.nlp_trained_model_path,\n",
    "            nlp_updated_base_model_path= prepare_base_model.nlp_updated_base_model_path,\n",
    "            training_data= training_data,\n",
    "            # mlflow_uri = MLFLOW_TRACKING_URI,\n",
    "            all_params = self.params,\n",
    "            params_batch_size= self.params.BATCH_SIZE,\n",
    "            params_epochs= self.params.EPOCHS,\n",
    "            params_learning_rate= self.params.LEARNING_RATE,\n",
    "            params_model_name=self.params.MODEL_NAME\n",
    "        )\n",
    "        \n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainerHF(object):\n",
    "    def __init__(self, config:TrainingConfig):\n",
    "        self.config = config\n",
    "        self.dataset = self.load_dataset()\n",
    "        self.tokenizer, self.model = self.load_model()\n",
    "        # self.set_loaders()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.set_seed()\n",
    "        \n",
    "    def load_model(self):\n",
    "        # mapping integer labels to string labels and vv\n",
    "        id2label = {0: 'Medical Necessity', 1: 'Experimental/Investigational', 2: 'Urgent Care'}\n",
    "        label2id = {'Medical Necessity': 0, 'Experimental/Investigational': 1, 'Urgent Care': 2}\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.params_model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                    self.config.params_model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id\n",
    "                )\n",
    "        print(model.classifier)\n",
    "        return tokenizer, model\n",
    "    \n",
    "    def _label_prep(self, row):\n",
    "        label_dict = {'Medical Necessity': 0, 'Experimental/Investigational': 1, 'Urgent Care': 2}\n",
    "        label_type = label_dict[row['Type']]\n",
    "        return {'labels': label_type}\n",
    "    \n",
    "    def _tokenize(self, row):\n",
    "        return self.tokenizer(row['Findings'],\n",
    "                            truncation=True,\n",
    "                            padding='max_length',\n",
    "                            max_length=50)\n",
    "                        \n",
    "    \n",
    "    def load_dataset(self):\n",
    "        dataset = dataset = load_dataset(path='csv',data_files=self.config.training_data + 'Independent_Medical_Reviews_Custom.csv',split=Split.TRAIN)\n",
    "        return dataset\n",
    "    \n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def set_loaders(self):\n",
    "        self.dataset = self.dataset.map(self._label_prep)\n",
    "        self.dataset = self.dataset.shuffle(seed=42)\n",
    "        self.dataset = self.dataset.train_test_split(test_size=0.2)\n",
    "        train_dataset = self.dataset['train']\n",
    "        test_dataset = self.dataset['test']\n",
    "        tokenized_train_dataset = train_dataset.map(self._tokenize, batched=True)\n",
    "        tokenized_test_dataset = test_dataset.map(self._tokenize, batched=True)\n",
    "\n",
    "        tokenized_train_dataset.set_format(\n",
    "                            type='torch',\n",
    "                            columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "        tokenized_test_dataset.set_format(\n",
    "                            type='torch',\n",
    "                            columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        \n",
    "        return tokenized_train_dataset, tokenized_test_dataset\n",
    "    \n",
    "    def load_trainer(self):\n",
    "        \n",
    "        # training_args = TrainingArguments(\n",
    "        #                 output_dir='output',\n",
    "        #                 num_train_epochs=2,\n",
    "        #                 per_device_train_batch_size=4,\n",
    "        #                 per_device_eval_batch_size=8,\n",
    "        #                 evaluation_strategy='steps',\n",
    "        #                 eval_steps=300,\n",
    "        #                 logging_steps=300,\n",
    "        #                 gradient_accumulation_steps=8,\n",
    "        #             )\n",
    "        tokenized_train_dataset, tokenized_test_dataset = self.set_loaders()\n",
    "        args = TrainingArguments(\n",
    "                self.config.nlp_trained_model_path + self.config.params_model_name,\n",
    "                # save_strategy=\"epoch\",\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                learning_rate=self.config.params_learning_rate,\n",
    "                per_device_train_batch_size=self.config.params_batch_size,\n",
    "                per_device_eval_batch_size=self.config.params_batch_size*2,\n",
    "                num_train_epochs=self.config.params_epochs,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"accuracy\",\n",
    "                logging_dir='logs',\n",
    "                remove_unused_columns=False,\n",
    "                gradient_accumulation_steps=8,\n",
    "            )\n",
    "        args.report_to = []\n",
    "        \n",
    "        def _compute_metrics(eval_pred):\n",
    "            predictions = eval_pred.predictions\n",
    "            labels = eval_pred.label_ids\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            return {\"accuracy\": (predictions == labels).mean()}\n",
    "        \n",
    "        # trainer = Trainer(\n",
    "        #                 self.model,\n",
    "        #                 args, \n",
    "        #                 train_dataset=self.dataset['train'],\n",
    "        #                 eval_dataset=self.dataset['test'],\n",
    "        #                 compute_metrics=_compute_metrics,\n",
    "        #                 tokenizer=self.tokenizer,\n",
    "        #             )\n",
    "        \n",
    "        trainer = Trainer(model=self.model,\n",
    "                args=args,\n",
    "                train_dataset=tokenized_train_dataset,\n",
    "                eval_dataset=tokenized_test_dataset,\n",
    "                compute_metrics=_compute_metrics)\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    def train(self):\n",
    "        trainer = self.load_trainer()\n",
    "        trainer.train()\n",
    "        self.save_checkpoint(trainer)\n",
    "    \n",
    "    \n",
    "    def evaluation(self):\n",
    "        self.load_checkpoint()\n",
    "        trainer = self.load_trainer()\n",
    "        outputs = trainer.evaluate()\n",
    "        print(outputs)\n",
    "        # y_true = outputs.label_ids\n",
    "        # y_pred = outputs.predictions.argmax(1)\n",
    "        # labels = self.dataset['train'].features['label'].names\n",
    "        # cm = confusion_matrix(y_true, y_pred)\n",
    "        # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "        # disp.plot(xticks_rotation=45)\n",
    "            \n",
    "    def save_checkpoint(self, trainer):\n",
    "        trainer.save_model(self.config.nlp_trained_model_path + self.config.params_model_name)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.config.nlp_trained_model_path + self.config.params_model_name)\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    \n",
    "    def predict(self, text):\n",
    "        self.load_checkpoint()\n",
    "        tokenized_text = self.tokenizer(text, return_tensors='pt')\n",
    "        tokenized_text.to(self.device)\n",
    "        outputs = self.model(**tokenized_text)\n",
    "        logits = outputs.logits\n",
    "        # model predicts one of the 1000 ImageNet classes\n",
    "        predicted_class_idx = logits.argmax(-1).item()\n",
    "        \n",
    "        return self.model.config.id2label[predicted_class_idx]\n",
    "    \n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metrics({'train_loss': np.mean(self.losses),'val_loss': np.mean(self.val_losses), 'train_accuracy': np.mean(self.accuracy), 'val_accuracy': np.mean(self.val_accuracy)})\n",
    "        \n",
    "            # Model registry does not work with file store\n",
    "            if tracking_url_type_store != \"file\":\n",
    "\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.pytorch.log_model(self.model, \"model\", registered_model_name=self.config.params_model_name)\n",
    "            else:\n",
    "                mlflow.pytorch.log_model(self.model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-07 15:04:44,217: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-05-07 15:04:44,220: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-05-07 15:04:44,221: INFO: common: created directory at: artifacts]\n",
      "[2024-05-07 15:04:44,222: INFO: common: created directory at: artifacts/training]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=3, bias=True)\n",
      "Urgent Care\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = configurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = ModelTrainerHF(config=training_config)\n",
    "    # training.train()\n",
    "    # training.evaluation()\n",
    "    prediction = training.predict('i need help with my heart burn')\n",
    "    print(prediction)\n",
    "    # training.log_into_mlflow()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
