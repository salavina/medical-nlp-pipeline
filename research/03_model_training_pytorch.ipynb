{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/milad/projects/medical-nlp-pipeline/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/milad/projects/medical-nlp-pipeline'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    nlp_trained_model_path: Path\n",
    "    nlp_updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    # mlflow_uri: str\n",
    "    all_params: dict\n",
    "    params_batch_size: int\n",
    "    params_epochs: int\n",
    "    params_learning_rate: float\n",
    "    params_model_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from medical_nlp.constants import *\n",
    "from medical_nlp.utils.common import read_yaml, create_directories\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# MLFLOW_TRACKING_URI = os.environ[\"MLFLOW_TRACKING_URI\"]\n",
    "# MLFLOW_TRACKING_USERNAME = os.environ[\"MLFLOW_TRACKING_USERNAME\"]\n",
    "# MLFLOW_TRACKING_PASSWORD = os.environ[\"MLFLOW_TRACKING_PASSWORD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class configurationManager:\n",
    "    def __init__(self,\n",
    "                 config_file_path=CONFIG_FILE_PATH,\n",
    "                 params_file_path=PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        model_training = self.config.model_training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        training_data = os.path.join(self.config.data_ingestion.root_dir, 'ca-independent-medical-review/')\n",
    "        \n",
    "        create_directories([model_training.root_dir])\n",
    "        \n",
    "        training_config = TrainingConfig(\n",
    "            root_dir= model_training.root_dir,\n",
    "            nlp_trained_model_path= model_training.nlp_trained_model_path,\n",
    "            nlp_updated_base_model_path= prepare_base_model.nlp_updated_base_model_path,\n",
    "            training_data= training_data,\n",
    "            # mlflow_uri = MLFLOW_TRACKING_URI,\n",
    "            all_params = self.params,\n",
    "            params_batch_size= self.params.BATCH_SIZE,\n",
    "            params_epochs= self.params.EPOCHS,\n",
    "            params_learning_rate= self.params.LEARNING_RATE,\n",
    "            params_model_name=self.params.MODEL_NAME\n",
    "        )\n",
    "        \n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-06 07:49:04,628: INFO: config: PyTorch version 2.2.0 available.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milad/miniconda3/envs/medical/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from datasets import load_dataset, Split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, ff_units, n_outputs, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.d_model = bert_model.config.dim\n",
    "        self.n_outputs = n_outputs\n",
    "        self.encoder = bert_model\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.d_model, ff_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_units, n_outputs)\n",
    "        )\n",
    "    def encode(self, source, source_mask=None):\n",
    "        states = self.encoder(\n",
    "        input_ids=source, attention_mask=source_mask)[0]\n",
    "        cls_state = states[:, 0]\n",
    "        return cls_state\n",
    "    def forward(self, X):\n",
    "        source_mask = (X > 0)\n",
    "        # Featurizer\n",
    "        cls_state = self.encode(X, source_mask)\n",
    "        # Classifier\n",
    "        out = self.mlp(cls_state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer(object):\n",
    "    def __init__(self, config:TrainingConfig, loss_fn=None, optimizer=None):\n",
    "        self.config = config\n",
    "        self.model = self.load_model()\n",
    "        self.loss_fn = loss_fn if loss_fn else nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optimizer if optimizer else optim.Adam(self.model.parameters(), lr=self.config.params_learning_rate)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.train_loader, self.val_loader = self.set_loaders()\n",
    "        \n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.total_epoches = 0\n",
    "        \n",
    "        self.train_step_fn = self._make_train_step_fn()\n",
    "        self.val_step_fn = self._make_val_step_fn()\n",
    "        \n",
    "    def load_model(self):\n",
    "        # return torch.load(self.config.nlp_updated_base_model_path)\n",
    "        bert_model = AutoModel.from_pretrained(self.config.params_model_name)\n",
    "        return BERTClassifier(bert_model, 128, n_outputs=1)\n",
    "    \n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def label_prep(self, row):\n",
    "        label_dict = {'Medical Necessity': 0, 'Experimental/Investigational': 1, 'Urgent Care': 2}\n",
    "        label_type = label_dict[row['Type']]\n",
    "        return {'labels': label_type}\n",
    "\n",
    "    \n",
    "    def set_loaders(self):\n",
    "        dataset = load_dataset(path='csv',data_files=self.config.training_data + 'Independent_Medical_Reviews_Custom.csv',split=Split.TRAIN)\n",
    "        dataset = dataset.map(self.label_prep)\n",
    "        shuffled_dataset = dataset.shuffle(seed=42)\n",
    "        split_dataset = shuffled_dataset.train_test_split(test_size=0.2)\n",
    "        train_dataset = split_dataset['train']\n",
    "        test_dataset = split_dataset['test']\n",
    "        auto_tokenizer = AutoTokenizer.from_pretrained(self.config.params_model_name)\n",
    "        tokenizer_kwargs = dict(truncation=True,\n",
    "                                padding=True,\n",
    "                                max_length=30,\n",
    "                                add_special_tokens=True)\n",
    "        \n",
    "        train_dataset_float = train_dataset.map(\n",
    "            lambda row: {'labels': [float(row['labels'])]}\n",
    "        )\n",
    "        test_dataset_float = test_dataset.map(\n",
    "            lambda row: {'labels': [float(row['labels'])]}\n",
    "        )\n",
    "        train_tensor_dataset = self._tokenize_dataset(train_dataset_float,\n",
    "                                                'Findings',\n",
    "                                                'labels',\n",
    "                                                auto_tokenizer,\n",
    "                                                **tokenizer_kwargs)\n",
    "        test_tensor_dataset = self._tokenize_dataset(test_dataset_float,\n",
    "                                                'Findings',\n",
    "                                                'labels',\n",
    "                                                auto_tokenizer,\n",
    "                                                **tokenizer_kwargs)\n",
    "        generator = torch.Generator()\n",
    "        train_loader = DataLoader(\n",
    "            train_tensor_dataset, batch_size=4,\n",
    "            shuffle=True, generator=generator\n",
    "        )\n",
    "        test_loader = DataLoader(test_tensor_dataset, batch_size=8)\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    # higher order function to be set and built globally and constructed the inner fuction without knowning x and y before hand\n",
    "    def _make_train_step_fn(self):\n",
    "        # single batch operation\n",
    "        def perform_train_step_fn(x,y):\n",
    "            # set the train mode\n",
    "            self.model.train()\n",
    "            \n",
    "            # step 1: compute model output\n",
    "            yhat = self.model(x)\n",
    "            \n",
    "            # step 2: compute the loss  \n",
    "            loss= self.loss_fn(yhat, y)\n",
    "            \n",
    "            # step 2': compute accuracy \n",
    "            yhat = torch.argmax(yhat,1)\n",
    "            total_correct = (yhat ==y).sum().item()\n",
    "            total = y.shape[0]\n",
    "            acc = total_correct/total\n",
    "            \n",
    "            # step 3: compute the gradient\n",
    "            loss.backward()\n",
    "            \n",
    "            #step4: update parameters\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            #step 5: return the loss\n",
    "            return loss.item() , acc\n",
    "        return perform_train_step_fn\n",
    "    \n",
    "    def _make_val_step_fn(self):\n",
    "        # single batch operation\n",
    "        def perform_val_step_fn(x,y):\n",
    "            # set the model in val mode\n",
    "            self.model.eval()\n",
    "            \n",
    "            #step 1: compute the prediction\n",
    "            yhat = self.model(x)\n",
    "            \n",
    "            #step 2: compute the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # step 2': compute accuracy \n",
    "            yhat = torch.argmax(yhat,1)\n",
    "            total_correct = (yhat ==y).sum().item()\n",
    "            total = y.shape[0]\n",
    "            acc = total_correct/total\n",
    "            \n",
    "            return loss.item(), acc\n",
    "        return perform_val_step_fn\n",
    "    \n",
    "    def _mini_batch(self, validation=False):\n",
    "        # one epoch operation \n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step_fn = self.val_step_fn\n",
    "            \n",
    "        else: \n",
    "            data_loader = self.train_loader\n",
    "            step_fn = self.train_step_fn\n",
    "            \n",
    "        if data_loader is None:\n",
    "            return None\n",
    "        \n",
    "        mini_batch_losses = []\n",
    "        mini_batch_accs = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "            \n",
    "            mini_batch_loss, mini_batch_acc = step_fn(x_batch,y_batch)\n",
    "            \n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "            mini_batch_accs.append(mini_batch_acc)\n",
    "        \n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        acc = np.mean(mini_batch_accs)\n",
    "        return loss, acc\n",
    "    \n",
    "    def train(self, seed=42):\n",
    "        self.set_seed(seed)\n",
    "        \n",
    "        for epoch in range(self.config.params_epochs):\n",
    "            self.total_epoches +=1\n",
    "            \n",
    "            # perform training on mini batches within 1 epoch\n",
    "            loss, acc = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "            self.accuracy.append(acc)\n",
    "            # now calc validation\n",
    "            with torch.no_grad():\n",
    "                val_loss, val_acc = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "                self.val_accuracy.append(val_acc)\n",
    "                \n",
    "            print(\n",
    "                f'\\nEpoch: {epoch+1} \\tTraining Loss: {loss:.4f} \\tValidation Loss: {val_loss:.4f}'\n",
    "            )\n",
    "            print(\n",
    "                f'\\t\\tTraining Accuracy: {100 * acc:.2f}%\\t Validation Accuracy: {100 * val_acc:.2f}%'\n",
    "            )\n",
    "        self.save_checkpoint()\n",
    "            \n",
    "    def save_checkpoint(self):\n",
    "        checkpoint = {'epoch': self.total_epoches,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'accuracy': self.accuracy,\n",
    "                      'val_loss': self.val_losses,\n",
    "                      'val_accuracy': self.val_accuracy\n",
    "                      }\n",
    "        torch.save(checkpoint, self.config.nlp_trained_model_path)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        checkpoint = torch.load(self.config.nlp_trained_model_path)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        self.total_epoches = checkpoint[\"epoch\"]\n",
    "        self.losses = checkpoint[\"loss\"]\n",
    "        self.accuracy = checkpoint['accuracy']\n",
    "        self.val_accuracy = checkpoint['val_accuracy']\n",
    "        self.val_losses = checkpoint[\"val_loss\"]\n",
    "        self.model.train() # always use train for resuming traning\n",
    "    \n",
    "    \n",
    "    # HF’s Dataset to Tokenized TensorDataset\n",
    "    def _tokenize_dataset(self, hf_dataset, sentence_field,\n",
    "        label_field, tokenizer, **kwargs):\n",
    "        sentences = hf_dataset[sentence_field]\n",
    "        token_ids = tokenizer(\n",
    "        sentences, return_tensors='pt', **kwargs\n",
    "        )['input_ids']\n",
    "        labels = torch.as_tensor(hf_dataset[label_field])\n",
    "        dataset = TensorDataset(token_ids, labels)\n",
    "        return dataset\n",
    "    \n",
    "    \n",
    "    def predict(self, text):\n",
    "        self.load_checkpoint()\n",
    "        self.model.eval()\n",
    "        auto_tokenizer = AutoTokenizer.from_pretrained(self.config.params_model_name)\n",
    "        tokenizer_kwargs = dict(truncation=True,\n",
    "                                padding=True,\n",
    "                                max_length=30,\n",
    "                                add_special_tokens=True)\n",
    "        tokenize_text = self._tokenize_dataset(text,\n",
    "                                                'sentence',\n",
    "                                                'labels',\n",
    "                                                auto_tokenizer,\n",
    "                                                **tokenizer_kwargs)\n",
    "        x_tensor = torch.as_tensor(tokenize_text).float()\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        \n",
    "        # set it back to the train mode\n",
    "        self.model.train()\n",
    "        labels = {0: 'Medical Necessity', 1: 'Experimental/Investigational', 2: 'Urgent Care'}\n",
    "        \n",
    "        return labels[np.argmax(y_hat_tensor.detach().cpu().numpy())]\n",
    "    \n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metrics({'train_loss': np.mean(self.losses),'val_loss': np.mean(self.val_losses), 'train_accuracy': np.mean(self.accuracy), 'val_accuracy': np.mean(self.val_accuracy)})\n",
    "        \n",
    "            # Model registry does not work with file store\n",
    "            if tracking_url_type_store != \"file\":\n",
    "\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.pytorch.log_model(self.model, \"model\", registered_model_name=\"nlp18Model\")\n",
    "            else:\n",
    "                mlflow.pytorch.log_model(self.model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-06 07:52:01,558: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-05-06 07:52:01,560: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-05-06 07:52:01,561: INFO: common: created directory at: artifacts]\n",
      "[2024-05-06 07:52:01,562: INFO: common: created directory at: artifacts/training]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 19225/19225 [00:00<00:00, 19475.01 examples/s]\n",
      "Map: 100%|██████████| 15380/15380 [00:01<00:00, 10644.00 examples/s]\n",
      "Map: 100%|██████████| 3845/3845 [00:00<00:00, 11381.82 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 \tTraining Loss: 0.1892 \tValidation Loss: 0.0086\n",
      "\t\tTraining Accuracy: 282.63%\t Validation Accuracy: 564.24%\n",
      "\n",
      "Epoch: 2 \tTraining Loss: -0.1376 \tValidation Loss: -0.3190\n",
      "\t\tTraining Accuracy: 282.63%\t Validation Accuracy: 564.24%\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = configurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = ModelTrainer(config=training_config)\n",
    "    training.train()\n",
    "    # training.log_into_mlflow()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
